{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Person Counter: Prediction\n",
    "This notebook contains code for prediction using pre-trained models. It stores the output in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"MOT16\"\n",
    "VIDEO_SEQ = 10 # Range 01 to 14\n",
    "MODEL_ID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "#from matplotlib import pyplot as plt # Commented because of warning that matplot lib is already loaded\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#from pc_utils import pc_PerImageEvaluation\n",
    "import pc_utils\n",
    "sys.path.append(\"../obj_det/\")\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variables\n",
    "\n",
    "We use models from the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Models used in paper\n",
    "ALL_MODEL = ['ssd_mobilenet_v1_coco_2017_11_17' #0\n",
    "    ,'ssd_inception_v2_coco_2017_11_17' #1\n",
    "    ,'rfcn_resnet101_coco_2018_01_28' #2\n",
    "    ,'faster_rcnn_resnet101_coco_2018_01_28' #3\n",
    "    ,'faster_rcnn_inception_v2_coco_2018_01_28' #4\n",
    "]\n",
    "\n",
    "MODEL_NAME = ALL_MODEL[MODEL_ID]\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "OD_DIR = '../obj_det'\n",
    "\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_CKPT = os.path.join(OD_DIR, PATH_TO_CKPT)\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "PATH_TO_LABELS = os.path.join(OD_DIR, PATH_TO_LABELS)\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "pc_label = {} # For label marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from normalized coordinates to original image coordinates\n",
    "def denormalise_box(box, image_size):\n",
    "    box[:,0] = box[:,0] * image_size[1]\n",
    "    box[:,1] = box[:,1] * image_size[0]\n",
    "    box[:,2] = box[:,2] * image_size[1]\n",
    "    box[:,3] = box[:,3] * image_size[0]\n",
    "    return box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "```\n",
    "MOT16\n",
    "/train\n",
    "  /MOT16-02\n",
    "    /seqinfo.ini\n",
    "    /img1\n",
    "    /gt\n",
    "        gt.txt\n",
    "\n",
    "PersonCounter\n",
    " /Output\n",
    "   /ModelA\n",
    "        prediction                 // Pickle file of groundtruth and prediction\n",
    "        /Image                     // Folder of images with GT and predicted BB\n",
    "        evaluate                   // Results of evalute\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if VIDEO_SEQ in [2,4,5,9,10,11,13]:\n",
    "    VIDEO_SEQ = str(VIDEO_SEQ).zfill(2)\n",
    "    PATH_TO_DATABASE = '../MOT16/train/MOT16-' + VIDEO_SEQ + '/'\n",
    "else:\n",
    "    VIDEO_SEQ = str(VIDEO_SEQ).zfill(2)\n",
    "    PATH_TO_DATABASE = '../MOT16/test/MOT16-' + VIDEO_SEQ + '/'\n",
    "\n",
    "PATH_TO_IMAGES_DIR = PATH_TO_DATABASE + 'img1/'\n",
    "PATH_TO_ANNOTATIONS_DIR = PATH_TO_DATABASE + '/gt/gt.txt'\n",
    "\n",
    "PATH_TO_OUTPUT_DIR = 'Output/'\n",
    "PATH_TO_PREDICTION_DIR = os.path.join(PATH_TO_OUTPUT_DIR, MODEL_NAME + '_MOT16_' + VIDEO_SEQ) # Output/Model_MOT16_01\n",
    "PREDICTION_PKL_FILE = os.path.join(PATH_TO_PREDICTION_DIR, \"prediction\")\n",
    "FILTERED_PKL_FILE = os.path.join(PATH_TO_PREDICTION_DIR, \"prediction_filtered\") # Filtered to only person class\n",
    "RESULT_CSV_FILE = os.path.join(PATH_TO_PREDICTION_DIR, \"dt.csv\") # Final result\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "# Store results to Output/Model directory\n",
    "if not os.path.exists(PATH_TO_PREDICTION_DIR):\n",
    "    os.makedirs(PATH_TO_PREDICTION_DIR)\n",
    "    os.makedirs(os.path.join(PATH_TO_PREDICTION_DIR,\"Image\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_np : np array of the image\n",
    "# prediction : prediction dictionary for the image\n",
    "# groundtruth : groundtruth dictionary for the image\n",
    "def visualize_image(image_np, prediction=None, groundtruth=None, isIDMode=False):\n",
    "    if isIDMode and prediction != None:\n",
    "        # Plot the prediction\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          prediction['detection_boxes'],\n",
    "          prediction['person_id'], #prediction['detection_classes'],\n",
    "          prediction['detection_scores'],\n",
    "          pc_label, #category_index,\n",
    "          instance_masks=prediction.get('detection_masks'),\n",
    "          use_normalized_coordinates=False,\n",
    "          min_score_thresh=0.30,\n",
    "          line_thickness=8)\n",
    "        return\n",
    "        \n",
    "    if prediction != None:\n",
    "        # Plot the prediction\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          prediction['detection_boxes'],\n",
    "          prediction['detection_classes'],\n",
    "          prediction['detection_scores'],\n",
    "          category_index,\n",
    "          instance_masks=prediction.get('detection_masks'),\n",
    "          use_normalized_coordinates=False,\n",
    "          min_score_thresh=0.30,\n",
    "          line_thickness=8)\n",
    "    \n",
    "    if groundtruth != None:\n",
    "        # Plot the ground truth\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          groundtruth['groundtruth_boxes'],\n",
    "          groundtruth['groundtruth_classes'],\n",
    "          None,\n",
    "          category_index,\n",
    "          instance_masks=groundtruth.get('detection_masks'),\n",
    "          use_normalized_coordinates=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path : file name with extension E.g: 00010.jpg\n",
    "# prediction : prediction dictionary of the image\n",
    "def drawBB(image_path, prediction, isIDMode=False):\n",
    "    # Visualization of the results of a detection.\n",
    "    original_image_path = os.path.join(PATH_TO_IMAGES_DIR, image_path)   \n",
    "    marked_image_path = os.path.join(PATH_TO_PREDICTION_DIR, 'Image')\n",
    "    marked_image_path = os.path.join(marked_image_path, image_path) \n",
    "\n",
    "    image = Image.open(original_image_path)\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # Update the image with predicted and groundtruth BB\n",
    "    #visualize_image(image_np, groundtruth=prediction)\n",
    "    visualize_image(image_np, prediction=prediction, isIDMode=isIDMode)    \n",
    "    im = Image.fromarray(image_np)\n",
    "    IMAGE_FILE = os.path.join(marked_image_path)\n",
    "    #print \"Save file in\" + IMAGE_FILE\n",
    "    im.save(IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract groundtruth from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gt_for_single_image(image_id):\n",
    "    annotation_path = os.path.join(PATH_TO_TEST_ANNOTATIONS_DIR, '{}.xml'.format(image_id))\n",
    "    \n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    groundtruth_dict = {}\n",
    "    gt_bbs = []\n",
    "    cat_id = []\n",
    "    cat_label = []\n",
    "    \n",
    "    for object_node in root.iterfind('object'):\n",
    "        # Extract boundary box from XML files\n",
    "        for bb in object_node.iterfind('bndbox'):\n",
    "            gt_bb = []\n",
    "            for val in ['ymin', 'xmin', 'ymax', 'xmax']:\n",
    "                gt_bb.append(float(bb.find(val).text))\n",
    "            gt_bbs.append(gt_bb)\n",
    "\n",
    "        # Extract ground truth category\n",
    "        child = object_node.find('name')\n",
    "        cat_label.append(child.text)\n",
    "        # Hardcoded as of now :/\n",
    "        if child.text == 'cow':\n",
    "            cat_id.append(21)\n",
    "        elif child.text == 'dog':\n",
    "            cat_id.append(18)\n",
    "        # print(child.text)\n",
    "\n",
    "    # Extract size\n",
    "    groundtruth_dict['size'] = [int(root.find('size/width').text), int(root.find('size/height').text)]\n",
    "    # Dog is category 18 / Todo update\n",
    "    groundtruth_dict['num_detections'] = len(gt_bbs)\n",
    "    groundtruth_dict['original_boxes'] = gt_bbs\n",
    "    groundtruth_dict['groundtruth_boxes'] = np.array(gt_bbs, dtype=\"float32\")\n",
    "    groundtruth_dict['groundtruth_classes'] = np.array(cat_id) # np.full([len(gt_bbs)], 18)\n",
    "    groundtruth_dict['groundtruth_class_labels'] = np.array(cat_label) # np.full([len(gt_bbs)], 'dog')\n",
    "    \n",
    "    return groundtruth_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV text-file containing one object instance per line. Each line must contain 10 values: \n",
    "# <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z> \n",
    "# All frame numbers, target IDs and bounding boxes are 1-based\n",
    "# Frame number is image number without leading 0\n",
    "# Person ID is <id>\n",
    "\n",
    "def extract_gt():\n",
    "    gt = 0\n",
    "    with open(PATH_TO_ANNOTATIONS_DIR, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            print ', '.join(row)\n",
    "            groundtruth = {}\n",
    "            gt_bbs = []\n",
    "            cat_label = []\n",
    "            \n",
    "            # For each BB in each image\n",
    "            gt_bb = []\n",
    "            gt_bb.append( float(row[3]) + float(row[5])  ) # ymin # bb_top - bb_height\n",
    "            gt_bb.append( float(row[2]) ) # xmin # bb_left\n",
    "            gt_bb.append( float(row[3]) ) # ymax # bb_top\n",
    "            gt_bb.append( float(row[2]) + float(row[4]) ) # xmax # bb_left + bb_width\n",
    "            \n",
    "            gt_bbs.append(gt_bb)\n",
    "            cat_label.append(str(category_index[1]['name']))\n",
    "            print cat_label\n",
    "            groundtruth['filename'] = row[1].zfill(6) + '.jpg'\n",
    "            groundtruth['groundtruth_boxes'] = np.array(gt_bbs, dtype=\"float32\")\n",
    "            groundtruth['groundtruth_classes'] = np.array(cat_label)\n",
    "\n",
    "            return groundtruth\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll the prediction BB as multiple row\n",
    "#<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>\n",
    "def appendMatching(image_id, prediction):    \n",
    "    for i in range(prediction['num_detections']):\n",
    "        row = []\n",
    "        # Frame number\n",
    "        row[0] = image_id\n",
    "        # ID\n",
    "        row[1] = prediction['person_id']\n",
    "        # bb_left\n",
    "        row[2] = prediction['detection_boxes'][i][1]\n",
    "        # bb_top\n",
    "        row[3] = prediction['detection_boxes'][i][2]\n",
    "        # bb_width\n",
    "        row[4] = prediction['detection_boxes'][i][3] - row[2]\n",
    "        # bb_height\n",
    "        row[5] = row[3] - prediction['detection_boxes'][i][0]\n",
    "        row = ''.join(row)\n",
    "        # conf, x, y, z\n",
    "        row = row + ',0,0,0,0'\n",
    "        #with open(RESULT_CSV_FILE,'a') as fd:\n",
    "        #    fd.write(row)\n",
    "        print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching algo\n",
    "Data association via IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON_COUNTER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match objects between two frames and tag id to new objects\n",
    "def twoFrameMatching(pie, dt, gt):\n",
    "    dt = pc_utils.matchOnIoU(pie, dt, gt)\n",
    "    # Assign id to unassigned detections\n",
    "    for i in range(dt['num_detections']):\n",
    "        if dt['person_id'][i] == -1:\n",
    "            global PERSON_COUNTER\n",
    "            PERSON_COUNTER += 1\n",
    "            global pc_label\n",
    "            pc_label[PERSON_COUNTER] = { 'id': PERSON_COUNTER, 'name' : 'PC'+ str(PERSON_COUNTER) }\n",
    "            dt['person_id'][i] = PERSON_COUNTER\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_groundtruth_classes = 1\n",
    "# matching_iou_threshold = 0.5\n",
    "# nms_iou_threshold = 1.0\n",
    "# nms_max_output_boxes = 10000\n",
    "\n",
    "# # Per image evaluation\n",
    "# pie = pc_utils.pc_PerImageEvaluation(num_groundtruth_classes, matching_iou_threshold, nms_iou_threshold,nms_max_output_boxes)\n",
    "\n",
    "# # Dataset\n",
    "# detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]], dtype=float)\n",
    "# detected_scores = np.array([0.6, 0.8, 0.5], dtype=float) # Original\n",
    "# dt = {}\n",
    "# dt['detection_boxes'] = detected_boxes\n",
    "# dt['detection_scores'] = detected_scores\n",
    "# dt['detection_classes'] = np.array([1,0,1])\n",
    "\n",
    "# groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 10, 10]], dtype=float)\n",
    "# gt = {}\n",
    "# gt['detection_boxes'] = groundtruth_boxes\n",
    "# gt['person_id'] = np.array([1,2,3])\n",
    "# PERSON_COUNTER = 3\n",
    "\n",
    "# #pc_utils.matchOnIoU(pie, dt, gt)\n",
    "# #twoFrameMatching(pie, dt, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign ID to person detected \n",
    "def assignID():\n",
    "    with open(FILTERED_PKL_FILE,'rb') as fd:\n",
    "        ev_data = pickle.load(fd)\n",
    "\n",
    "    # Init per image evaluation\n",
    "    num_groundtruth_classes = 1\n",
    "    matching_iou_threshold = 0.5\n",
    "    nms_iou_threshold = 1.0\n",
    "    nms_max_output_boxes = 10000\n",
    "\n",
    "    pie = pc_utils.pc_PerImageEvaluation(num_groundtruth_classes, matching_iou_threshold, nms_iou_threshold,nms_max_output_boxes)\n",
    "\n",
    "    # First frame\n",
    "    i = 1\n",
    "    image_id = str(i).zfill(6)\n",
    "    gt = ev_data[image_id] # treated as gt\n",
    "\n",
    "    # Initialize the person counter\n",
    "    global PERSON_COUNTER\n",
    "    PERSON_COUNTER = 0\n",
    "    gt['person_id'] = np.array([i+1 for i in range(gt['num_detections'])])\n",
    "    \n",
    "    for i in range(gt['num_detections']):\n",
    "        global pc_label\n",
    "        pc_label[i] = { 'id': i, 'name' : 'PC'+ str(i)}\n",
    "    \n",
    "    image_id = str(i+1).zfill(6)\n",
    "    dt = ev_data[image_id] # treated as dt\n",
    "\n",
    "    while i < 5: # Upto last but one    \n",
    "        # Returns gt for next\n",
    "        gt = twoFrameMatching(pie, dt, gt)\n",
    "        # Prepare next loop\n",
    "        i = i + 1\n",
    "        image_id = str(i+1).zfill(6)\n",
    "        dt = ev_data[image_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from object_detection.utils import np_box_list\n",
    "# from object_detection.utils import np_box_list_ops\n",
    "\n",
    "# detected_boxlist = np_box_list.BoxList(detected_boxes)\n",
    "# print detected_boxlist\n",
    "# detected_boxlist.add_field('scores', detected_scores)\n",
    "\n",
    "# print detected_boxlist.get()\n",
    "# print detected_scores\n",
    "# detected_boxlist = np_box_list_ops.non_max_suppression(detected_boxlist, pie.nms_max_output_boxes, pie.nms_iou_threshold)\n",
    "\n",
    "# print detected_boxlist.get()\n",
    "# print detected_boxlist.get_field('scores')\n",
    "\n",
    "# After sort\n",
    "# print \"After sort\"\n",
    "# boxlist = np_box_list_ops.sort_by_field(detected_boxlist, 'scores')\n",
    "# print boxlist.get()\n",
    "# scores = boxlist.get_field('scores')\n",
    "# print scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter prediction to only person class\n",
    "def filter_prediction():\n",
    "    print(PREDICTION_PKL_FILE)\n",
    "    with open(PREDICTION_PKL_FILE,'rb') as fd:\n",
    "        ev_data = pickle.load(fd)\n",
    "        # Need to sequentially analyse\n",
    "        for i in range(1,655): # 655\n",
    "            # File name without extension\n",
    "            image_id = str(i).zfill(6)\n",
    "            prediction = ev_data[image_id]\n",
    "            # Person class = 1 in COCO dataset\n",
    "            idx = prediction['detection_classes'] == 1\n",
    "            prediction['num_detections'] = np.count_nonzero(idx)\n",
    "            prediction['detection_boxes'] = prediction['detection_boxes'][idx, :]\n",
    "            prediction['detection_scores'] = prediction['detection_scores'][idx]\n",
    "            prediction['detection_classes'] = prediction['detection_classes'][idx]\n",
    "            print \"Image\", image_id, \"dt\", prediction['num_detections']\n",
    "        # Store in pickle\n",
    "        with open(FILTERED_PKL_FILE,'wb') as fd2:\n",
    "            pickle.dump(ev_data, fd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the prediction\n",
    "def evaluate():\n",
    "    print(FILTERED_PKL_FILE)\n",
    "    with open(FILTERED_PKL_FILE,'rb') as fd:\n",
    "        ev_data = pickle.load(fd)\n",
    "        # Need to sequentially analyse\n",
    "        for i in range(1,5): #655\n",
    "            # File name without extension\n",
    "            image_id = str(i).zfill(6)\n",
    "            prediction = ev_data[image_id]\n",
    "            #print \"Image\", i\n",
    "            #print prediction['detection_boxes']\n",
    "            # Plot the BB on image\n",
    "            \n",
    "            # Give dummy person id\n",
    "            global pc_label\n",
    "            for i in range(prediction['num_detections']):\n",
    "                pc_label[i] = { 'id': i, 'name' : 'PC'+ str(i) }\n",
    "            prediction['person_id'] = np.array([i+1 for i in range(prediction['num_detections'])])\n",
    "            \n",
    "            image_path = image_id + '.jpg'\n",
    "            drawBB(image_path, prediction, isIDMode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #filter_prediction()\n",
    "    #ev_data = evaluate()\n",
    "    evaluate()\n",
    "    print \"Done\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
